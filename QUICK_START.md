# ğŸš€ Quick Start Guide - Exoplanet Detection System

Get your complete AI-powered exoplanet detection system running in **3 simple steps**!

---

## âš¡ TL;DR - Start Everything Now

```bash
# Start all services
./START_SYSTEM.sh

# Visit the app
# Open browser: http://localhost:3000
```

That's it! The system will automatically start:
- âœ… Backend API (FastAPI)
- âœ… Frontend UI (React)
- âœ… AI Chat (Ollama - if installed)

---

## ğŸ“‹ Prerequisites

### Required
- **Python 3.12+** (you have this)
- **Node.js & npm** (for frontend)
- **pip** (Python package manager)

### Optional (for AI Chat)
- **Ollama** - Local LLM for chat features

---

## ğŸ¯ Step-by-Step Setup

### 1ï¸âƒ£ Install Dependencies

```bash
# Install Python dependencies
pip install -r requirements.txt

# Install LLM dependencies (optional, for chat)
pip install -r requirements-llm.txt

# Install frontend dependencies
cd frontend
npm install
cd ..
```

### 2ï¸âƒ£ Train Models (First Time Only)

```bash
# Train on Kepler dataset (~2 minutes)
python3 backend/models/train_pipeline.py
```

Expected output:
```
âœ“ XGBoost: 98.95% accuracy (best)
âœ“ LightGBM: 98.75%
âœ“ Random Forest: 98.62%
âœ“ Neural Network: 98.42%
âœ“ Gradient Boosting: 98.42%
```

### 3ï¸âƒ£ Install Ollama (Optional - for Chat Features)

```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull the model
ollama pull llama3.2
```

---

## ğŸƒ Running the System

### Option A: **Automatic Start (Recommended)**

```bash
./START_SYSTEM.sh
```

This script:
- âœ… Checks all prerequisites
- âœ… Starts backend, frontend, and Ollama
- âœ… Shows you all access URLs
- âœ… Creates log files

### Option B: **Manual Start (3 Terminals)**

**Terminal 1 - Backend:**
```bash
python3 backend/api/main.py
```

**Terminal 2 - Frontend:**
```bash
cd frontend
npm run dev
```

**Terminal 3 - Ollama (optional):**
```bash
ollama serve
```

---

## ğŸŒ Access the System

Once running, open these URLs:

| Service | URL | Description |
|---------|-----|-------------|
| ğŸ¨ **Main App** | http://localhost:3000 | Full web interface |
| ğŸ“š **API Docs** | http://localhost:8000/docs | Interactive API docs |
| ğŸ’š **Health Check** | http://localhost:8000/health | System status |
| ğŸ’¬ **Chat Status** | http://localhost:8000/chat/status | AI chat status |

---

## ğŸ® What You Can Do

### 1. **Dashboard**
View system overview, dataset statistics, and model performance

### 2. **Make Predictions**
- **Single:** Enter exoplanet parameters manually
- **Batch:** Test multiple samples at once
- **Upload:** Upload CSV files for bulk predictions

### 3. **Train Models**
- Select dataset (Kepler, TESS, K2)
- Watch real-time training progress
- Compare model performance

### 4. **Explore Data**
- Browse NASA datasets
- View statistics and distributions
- Examine sample data

### 5. **Manage Models**
- View all trained models
- Switch between models
- See performance metrics

### 6. **AI Assistant** âœ¨
Ask questions like:
- "How does the transit method work?"
- "Which model is most accurate?"
- "Compare Kepler and TESS datasets"
- "Explain this false positive classification"

---

## ğŸ›‘ Stopping the System

```bash
./STOP_SYSTEM.sh
```

Or manually press `Ctrl+C` in each terminal.

---

## ğŸ§ª Test the System

Run comprehensive tests:

```bash
python3 test_system.py
```

This will verify:
- âœ… Models are trained
- âœ… Predictions work
- âœ… Light curves load
- âœ… Datasets load
- âœ… API is healthy
- âœ… Chat system works

---

## ğŸ“Š System Status Check

### Check if Backend is Running
```bash
curl http://localhost:8000/health
```

Expected response:
```json
{
  "status": "healthy",
  "models_loaded": true,
  "datasets_available": ["kepler", "tess", "k2"]
}
```

### Check if Frontend is Running
```bash
curl http://localhost:3000
```

Should return HTML.

### Check if Ollama is Running
```bash
curl http://localhost:11434/api/version
```

---

## ğŸ› Troubleshooting

### Backend won't start

**Problem:** Port 8000 already in use
```bash
# Find what's using the port
lsof -i:8000

# Kill it
kill -9 <PID>
```

**Problem:** Models not found
```bash
# Check if models exist
ls -lh data/models/

# Retrain if needed
python3 backend/models/train_pipeline.py
```

### Frontend won't start

**Problem:** Node modules missing
```bash
cd frontend
rm -rf node_modules package-lock.json
npm install
```

**Problem:** Port 3000 in use
```bash
# Edit frontend/vite.config.js
# Change port to 3001
```

### Chat not working

**Problem:** Ollama not running
```bash
# Check if Ollama is running
curl http://localhost:11434/api/version

# Start it
ollama serve
```

**Problem:** Model not pulled
```bash
# Check available models
ollama list

# Pull llama3.2 if missing
ollama pull llama3.2
```

---

## ğŸ“ Project Structure

```
exoplanet/
â”œâ”€â”€ backend/              # Python backend
â”‚   â”œâ”€â”€ api/             # FastAPI endpoints
â”‚   â”œâ”€â”€ models/          # ML models & training
â”‚   â”œâ”€â”€ utils/           # Data loading, preprocessing
â”‚   â””â”€â”€ llm/             # Ollama & RAG system
â”‚
â”œâ”€â”€ frontend/            # React frontend
â”‚   â””â”€â”€ src/
â”‚       â”œâ”€â”€ pages/       # 6 pages
â”‚       â””â”€â”€ services/    # API client
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/            # NASA datasets (CSV)
â”‚   â”œâ”€â”€ processed/      # Preprocessed data
â”‚   â””â”€â”€ models/         # Trained models
â”‚
â”œâ”€â”€ lightcurve/         # 203 light curve files
â”‚
â””â”€â”€ logs/               # Runtime logs
```

---

## ğŸ”¥ Pro Tips

1. **First time?** Run `./START_SYSTEM.sh` - it does everything for you

2. **Training slow?** The first training takes ~2 minutes. Subsequent trainings are cached.

3. **Chat not working?** It's optional! The system works fine without Ollama. Chat just adds extra features.

4. **Want to customize?** Check `backend/config/config.py` for all settings.

5. **Logs?** All logs are in the `logs/` directory:
   - `logs/backend.log` - API logs
   - `logs/frontend.log` - Frontend logs
   - `logs/ollama.log` - Chat logs

6. **Performance?** The system can process:
   - Single predictions: <100ms
   - Batch: ~10,000 samples/minute
   - Chat responses: 1-3 seconds

---

## ğŸ“š Next Steps

- **Read the full documentation:** [COMPLETE_SYSTEM_GUIDE.md](COMPLETE_SYSTEM_GUIDE.md)
- **Explore the API:** http://localhost:8000/docs
- **Try the chat assistant:** Ask about exoplanet detection methods
- **Upload your own data:** Use the Predict page to upload CSV files
- **Train on different datasets:** Try TESS and K2 datasets

---

## ğŸ†˜ Need Help?

- **Documentation:** See `Documentation/` folder
- **API Reference:** http://localhost:8000/docs
- **System Overview:** [COMPLETE_SYSTEM_GUIDE.md](COMPLETE_SYSTEM_GUIDE.md)
- **Test Script:** Run `python3 test_system.py`

---

## âœ… Verification Checklist

Before you start using the system:

- [ ] Python dependencies installed (`pip install -r requirements.txt`)
- [ ] Models trained (`python3 backend/models/train_pipeline.py`)
- [ ] Frontend dependencies installed (`cd frontend && npm install`)
- [ ] Backend running (http://localhost:8000/health returns healthy)
- [ ] Frontend running (http://localhost:3000 loads)
- [ ] Ollama installed and running (optional, for chat)

---

## ğŸ‰ You're All Set!

Your complete exoplanet detection system is ready to:
- ğŸ” Detect exoplanets with 98.95% accuracy
- ğŸ“Š Analyze NASA mission data
- ğŸ’¬ Answer questions with AI
- ğŸš€ Process thousands of predictions per minute

**Happy exoplanet hunting! ğŸªâœ¨**

---

*Built for NASA Space Apps Challenge 2025*
